{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 자바 설치후 자바가 설치된 폴더 설정\n",
    "os.environ[\"JAVA_HOME\"] = \"C:/bigdata/jdk-18.0.2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# 자바의 실행 파일 java.exe가 존재하는 경로를 path에 추가\n",
    "sys.path.append(\"C:/bigdata/jdk-18.0.2/bin/java.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 형태소/단어 분리를 위한 라이브러리 설치\n",
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소/단어 분리에 도움을 주는 라이브러리 설치\n",
    "!pip install Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일에 저장되어 있는 데이터를 불러옴\n",
    "youtube = pd.read_csv(\"환경부_웹_크롤링.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use ('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# 이상치 줄이기 전 그래프\n",
    "sns.distplot(youtube['view_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 유튜브 조회수 데이터에 최근에 올라온 영상이나 특이하게 많이 시청된 영상을 이상치로 판단하고 제거\n",
    "def outliers(data):\n",
    "    # Q!, Q3, IQR을 설정\n",
    "    q1, q3 = np.percentile(data, [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - (iqr * 1.5)\n",
    "    upper_bound = q3 + (iqr * 1.5)\n",
    "    \n",
    "    # 설정한 범위를 넘어가면 이상치라 판단\n",
    "    return np.where((data > upper_bound)|(data < lower_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outliers 함수에 유튜브 조회수 데이터를 넣고 이상치 판단\n",
    "outlier_index = outliers(youtube['view_num'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이상치가 아닌 데이터를 넣은 리스트를 만듦\n",
    "not_outlier = []\n",
    "# 유튜브 조회수 데이터에서 이상치가 아닌 데이터가 있으면 위의 리스트에 넣음\n",
    "for i in youtube.index:\n",
    "    if i not in outlier_index:\n",
    "        not_outlier.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# youtube 데이터에서 이상치가 아닌 데이터를 지정\n",
    "youtube = youtube.loc[not_outlier]\n",
    "# 그 외의 것들은 drop하고 인덱스를 재설정함\n",
    "youtube = youtube.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# 이상치 제거 후 그래프\n",
    "sns.distplot(youtube['view_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = Twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트를 전처리 하는 함수를 만듦\n",
    "def preprocessingText(text):\n",
    "    # 전처리한 결과를 저장할 리스트\n",
    "    stems = []\n",
    "    # text를 단어로 나누고 품사를 저장\n",
    "    # stem = True: 단어의 기본형으로 변환\n",
    "    tagged_review = twitter.pos(text, stem = True)\n",
    "    \n",
    "    # 단어는 word, 품사는 pos에 대입\n",
    "    for word, pos in tagged_review:\n",
    "        # 품사가 명사, 형용사, 동사, 부사이면 stems에 추가\n",
    "        if pos == \"Noun\" or pos == \"Adjective\" or pos ==\"Verb\" or pos == \"Adverb\":\n",
    "            stems.append(word)\n",
    "    \n",
    "    # 리스트인 stems를 문자열로 만들어 리턴\n",
    "    return \" \".join(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유튜브 제목을 데이터 처리하여 넣을 열을 추가하고 초기값을 결측치로 설정\n",
    "import numpy as np\n",
    "youtube[\"title_convert\"] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# title 행의 개수만큼 반복\n",
    "for index in tqdm(range(len(youtube['title']))):\n",
    "    try:\n",
    "        # title 행의 데이터를 title_convert에 대입하고, 함수를 사용하여 단어 리턴\n",
    "        title = youtube.loc[index, \"title\"]\n",
    "        youtube.loc[index, \"title_convert\"] = preprocessingText(title)\n",
    "        \n",
    "    except Exception as e:\n",
    "        # 에러가 발생했을 때 결측치 대입\n",
    "        youtube.loc[index, \"title_convert\"] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title_convert에 글자수가 1 이상인 행만 저장(한글이 아닌 모든 데이터를 삭제 - 영어, 중국어, 이모티콘 등)\n",
    "youtube = youtube[youtube[\"title_convert\"].str.len() > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중간중간에 index 숫자가 빠져있으므로, 기존 index 제거\n",
    "youtube.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title_convert_list 열을 만들어서 title_convert의 데이터를 공백을 기준으로 단어별로 분리해서 리턴\n",
    "youtube[\"title_convert_list\"] = youtube[\"title_convert\"].apply(lambda data :data.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1, 2, 3 분위수, IQR, 평균 지정\n",
    "Q1 = youtube['view_num'].quantile(.25)\n",
    "Q2 = youtube['view_num'].quantile(.5)\n",
    "Q3 = youtube['view_num'].quantile(.75)\n",
    "IQR = Q3 - Q1\n",
    "mean = youtube['view_num'].mean()\n",
    "max = youtube['view_num'].max()\n",
    "min = youtube['view_num'].min()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_num_list = []\n",
    "\n",
    "# youtube 데이터의 행의 수만큼 반복\n",
    "for index in range(len(youtube)):\n",
    "    view_num = youtube['view_num'][index]\n",
    "    # 1분위수보다 작다면 0으로 치환 후 리스트에 저장\n",
    "    if(view_num < mean):\n",
    "        view_num_list.append(0)\n",
    "\n",
    "    else:\n",
    "        view_num_list.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 열에 리스트 대입\n",
    "youtube[\"view_num_convert\"] = view_num_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 전체 데이터중 80% 는 학습 데이터, 20%는 테스트 데이터로 추출\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "youtube['title_convert_list'], # 독립변수\n",
    "youtube['view_num_convert'], # 종속변수\n",
    "test_size = 0.2, # 테스트 사이즈 지정\n",
    "stratify = youtube['view_num_convert'], # 데이터 분리시 0, 1, 2, 3의 빈도수 비율 그대로 분리\n",
    "random_state = 1 # random_state를 지정해주지 않으면 실행할때마다 다른 결과가 나옴\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 조회수 높은 영상에서 자주 사용되는 단어 빈도를 찾아보기 위해 조회수 높은 영상만 출력\n",
    "under = youtube[youtube[\"view_num_convert\"] == 0].index\n",
    "youtube_over = youtube.drop(under)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 조회수 높은 영상에서 사용된 모든 단어 출력\n",
    "for line in youtube_over['title_convert_list']:\n",
    "    print(\" \".join(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 전체 내용에서 단어가 한번 발생하면 리스트에 추가해주고, 추가적으로 발생하면 원래 단어에 더해주는 함수 구현\n",
    "wordCount = {}\n",
    "for line in youtube_over['title_convert_list']:\n",
    "    line_str = \" \".join(line)\n",
    "\n",
    "    wordList = line_str.split()\n",
    "\n",
    " \n",
    "    for word in wordList:\n",
    "        wordCount[word] = int(wordCount.get(word, 0) + 1 )\n",
    "        keys = sorted(wordCount.keys())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최다 빈도수 단어부터 시작해서 내림차순으로 정렬\n",
    "sort_count = sorted(wordCount.items(), key = lambda x:x[1], reverse=True)\n",
    "# 시각화를 위해 제일 많이 사용된 단어 25개를 가져옴\n",
    "converted_dict = dict(sort_count[:25])\n",
    "print(converted_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 조회수가 많은 영상에서 최다 빈도 단어를 바 그래프로 그림\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.family'] ='Malgun Gothic'\n",
    "\n",
    "matplotlib.rcParams['axes.unicode_minus'] =False\n",
    "\n",
    "keys = converted_dict.keys()\n",
    "values = converted_dict.values()\n",
    "\n",
    "parameters = {\n",
    "              'axes.labelsize': 50,\n",
    "              'axes.titlesize': 35,\n",
    "              'xtick.labelsize': 40,\n",
    "              'ytick.labelsize': 40\n",
    "              }\n",
    "plt.rcParams.update(parameters)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (60,30))\n",
    "plt.bar(keys, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제목과 조회수를 전처리한 데이터 외의 다른 내용들은 삭제\n",
    "youtube = youtube.drop(['view_num'], axis = 1)\n",
    "youtube = youtube.drop(['title'], axis = 1)\n",
    "youtube = youtube.drop(['title_convert'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터프레임을 파일로 저장\n",
    "youtube.to_csv(\"환경부_데이터_전처리.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# 각 단어들을 학습해서 Vector 형태로 생성\n",
    "word2vec = Word2Vec(X_train, # 리스트 형태의 X_train 데이터\n",
    "                   sg = 2, # word2vec 의 skip-gram 형식 사용\n",
    "                   window = 5, # 고려할 앞뒤 폭 (앞뒤 5개 단어)\n",
    "                   min_count = 2, # 2회 이하 사용된 단어 무시)\n",
    "                    workers = 10 # 동시에 처리학 작업 수\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 워드 벡터의 전체 단어 수와 각 벡터의 크기를 저장\n",
    "num_words, emb_dim = word2vec.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2cSorted=dict(sorted(w2c.items(), key=lambda x: x[1],reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install protobuf==3.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dropout, LSTM, Bidirectional, Dense, Embedding, Flatten, SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스를 입력하면 해당 인덱스의 단어를 벡터 값으로 변환하는 객체\n",
    "emb = Embedding(input_dim = num_words, output_dim = emb_dim,\n",
    "               trainable = False, weights = [word2vec.wv.vectors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫번째 단어를 벡터화 한 데이터\n",
    "word2vec.wv.vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선형회귀를 진행하는 모델 생성 후, 위의 객체 추가\n",
    "seq = Sequential()\n",
    "seq.add(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec에 저장된 단어들을 Dictionary 객체로 리턴\n",
    "word_vector_keys = list(word2vec.wv.key_to_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train에 저장된 문자열을 인덱스로 변환해서 저장할 배열\n",
    "X_train_int= []\n",
    "\n",
    "# X_train의 각 행을 column에 저장\n",
    "for column in tqdm(X_train):\n",
    "    input = []\n",
    "    \n",
    "    # 단어가 Dictionary에 저장되어 있으면 input에 추가\n",
    "    for word in column:\n",
    "        if word in word_vector_keys:\n",
    "            input.append(word2vec.wv.index_to_key.index(word))\n",
    "\n",
    "    X_train_int.append(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test에 저장된 문자열을 인덱스로 변환하여 저장할 배열\n",
    "X_test_int= []\n",
    "\n",
    "# X_test의 각 행을 column에 저장\n",
    "for column in tqdm(X_test):\n",
    "    input = []\n",
    "    \n",
    "    # 단어가 Dictionary에 저장되어 있으면 input에 추가\n",
    "    for word in column:\n",
    "        if word in word_vector_keys:\n",
    "            input.append(word2vec.wv.index_to_key.index(word))\n",
    "\n",
    "    X_test_int.append(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "\"\"\"각 제목의 길이가 다름.\n",
    "딥러닝을 하기 위해서는 길이를 통일할 필요가 있으므로, 단어 20글자를 기준으로 잡아 데이터를 채우고, 빈 영역은 0으로 채움\n",
    "만약 제목 단어의 수가 20보다 길다면, 뒤에서부터 단어 20개 잘라서 맞춤(단어 개수가 20개 넘는 제목이 거의 없음)\n",
    "\"\"\"\n",
    "# train, test 에서 둘다 실행\n",
    "X_train_pad = sequence.pad_sequences(X_train_int, maxlen = 20)\n",
    "X_test_pad = sequence.pad_sequences(X_test_int, maxlen = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 조회수를 0, 1으로 변환한 객체의 개수들\n",
    "count_0 = youtube[\"view_num_convert\"].value_counts()[0]\n",
    "count_1 = youtube[\"view_num_convert\"].value_counts()[1]\n",
    "\n",
    "# 전체 데이터의 갯수\n",
    "total = len(youtube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 레벨의 가중치 설정\n",
    "# 전체 데이터 수 (각 데이터의 개수 * 레벨 종류의 합)\n",
    "weight_for_0 = total / (count_0 * 2)\n",
    "weight_for_1 = total / (count_1 * 2)\n",
    "\n",
    "# 가중치 설정한 것을 class_weight으로 지정\n",
    "class_weight = {\n",
    "                    0: weight_for_0,\n",
    "                    1: weight_for_1\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 워드벡터 형식으로 바꾼 데이터를 선형회귀하는 모델을 만듦\n",
    "# 양뱡향 LSTM과 Dense layer를 통해 선형회귀를 진행\n",
    "# 0, 1의 결과로 선형회귀 하기 때문에, 마지막 layer 로 sigmoid를 사용\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim = num_words, output_dim = emb_dim,\n",
    "                   trainable = False, weights = [word2vec.wv.vectors]))\n",
    "model.add(Bidirectional(LSTM(512, recurrent_dropout = 0.2, return_sequences = True)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Bidirectional(LSTM(256, recurrent_dropout = 0.2, return_sequences = True)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Bidirectional(LSTM(128, recurrent_dropout = 0.2, return_sequences = True)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Bidirectional(LSTM(64, recurrent_dropout = 0.2, return_sequences = True)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Bidirectional(LSTM(64, recurrent_dropout = 0.2)))\n",
    "model.add(Dense(512, activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(256, activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 모델을 학습에 적용시킴\n",
    "model.compile(loss = \"binary_crossentropy\",\n",
    "             optimizer = Adam(learning_rate = 1e-3),\n",
    "             metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 적합할 때 시간을 단축시키기 위해 형식을 바꿔줌\n",
    "y_train = np.array(y_train, dtype = \"float32\")\n",
    "y_test = np.array(y_test, dtype = \"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습시 가장 정확도가 높은 모델을 저장하도록 설정\n",
    "cb_checkpoint = ModelCheckpoint(filepath=\"C:/bigdata/natual_language_processing_1.h5\", # 학습 진행시 가장 정확도가 높은 모델을 저장할 경로\n",
    "                                monitor='loss', # 저장할 조건 val_acc (테스트 데이터의 loss) 가 가장 낮은 모델을 저장\n",
    "                                vervose=1, #함수의 진행 과정 출력\n",
    "                                save_best_only=True # 가장 정확도가 높은 모델 1개만 저장\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습을 이어서 할때, 저장된 weight를 불러옴\n",
    "model.load_weights(\"C:/bigdata/natual_language_processing_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(X_train_pad, y_train, #X_train_pad, y_train 을 통해 학습\n",
    "                 batch_size = 1000,\n",
    "                 epochs = 100,\n",
    "                 class_weight = class_weight, # 지정했던 weight 사용\n",
    "                 callbacks=[cb_checkpoint], # 학습시 가장 loss가 낮은 모델을 저장하도록 설정 \n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split으로 만들었던 테스트 데이터로 검사\n",
    "model.evaluate(X_test_pad, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유튜브 제목을 임의로 정해서 테스트\n",
    "my_test_data = [\n",
    "    '''\n",
    "    환경부와 함께하는 환경의 날 현장 이야기\n",
    "    '''\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로 만든 제목 또한 워드 벡터 형식으로 바꿔서 테스트 실행\n",
    "test_data_list = []\n",
    "\n",
    "for my_data in my_test_data:\n",
    "    text = preprocessingText(my_data)\n",
    "    test_data_list.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새롭게 만든 테스트 데이터를 새로운 리스트에 넣음\n",
    "my_test_token = []\n",
    "for column in test_data_list:\n",
    "    token_to_index = []\n",
    "    for word in column.split():\n",
    "        if word in word2vec.wv.index_to_key:\n",
    "            word_index = word2vec.wv.index_to_key.index(word)\n",
    "            token_to_index.append(word_index)\n",
    "            \n",
    "    my_test_token.append(token_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 테스트 데이터 또한 같은 형식으로 맞춰줌\n",
    "my_test_pad = sequence.pad_sequences(my_test_token, maxlen = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 통해 새로운 테스트 데이터를 검사하고, 조회수가 평균보다 높은지 낮은지 판단\n",
    "predict = model.predict(my_test_pad)\n",
    "if predict < 0.5:\n",
    "    print(\"조회수 평균보다 낮을 것으로 예상\")\n",
    "else:\n",
    "    print(\"조회수 평균보다 높을 것으로 예상\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label = 'train loss')\n",
    "\n",
    "\n",
    "acc_ax.plot(hist.history['acc'], 'b', label = 'train acc')\n",
    "\n",
    "\n",
    "loss_ax.set_xlabel('epochs')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('acc')\n",
    "\n",
    "loss_ax.set_ylim([0.69, .70])\n",
    "acc_ax.set_ylim([0.0, 1.0])\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
